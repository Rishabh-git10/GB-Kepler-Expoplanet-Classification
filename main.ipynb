{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing librabries\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing an empty dataframe for storing the data after each iteration\n",
    "results_df = pd.DataFrame(columns=['n_estimators', 'learning_rate', 'max_depth', 'max_features', 'mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "file_path = \"./dataset.csv\"\n",
    "try:\n",
    "    dataset = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please check the path and try again.\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify if dataset is a dataframe\n",
    "if not isinstance(dataset, pd.DataFrame):\n",
    "    print(\"Dataset is not a dataframe. Please check the file and try again.\")\n",
    "    exit(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(dataset['koi_disposition'])\n",
    "\n",
    "dataset_numeric = dataset.dropna(subset=['koi_score'])\n",
    "\n",
    "#Save the 'koi_disposition' column in a variable and drop it from the dataset\n",
    "koi_disposition_column = dataset['koi_disposition']\n",
    "\n",
    "non_numeric_columns = dataset.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "#Drop 'koi_disposition' and any other non-numeric columns from the dataset\n",
    "dataset_numeric = dataset.drop(columns=non_numeric_columns)\n",
    "\n",
    "dataset_numeric['koi_disposition'] = koi_disposition_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_numeric.drop(columns=['koi_disposition']), y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle missing values using SimpleImputed for the training and testing sets\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the training and testing sets\n",
    "scaler = StandardScaler().fit(X_train_imputed)\n",
    "X_train_scaled = scaler.transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting initial ranges for hyperparameters\n",
    "param_ranges = {\n",
    "    'n_estimators': (90, 130),\n",
    "    'learning_rate': (0.01, 0.1),\n",
    "    'max_depth': (5, 15),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "#Setting the number of iterations\n",
    "n_iter = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/200\n",
      "Sampled Parameters: {'n_estimators': 99, 'learning_rate': 0.06236361197257111, 'max_depth': 6, 'max_features': None}\n",
      "Mean Score: 0.9163617354939885\n",
      "Iteration 2/200\n",
      "Sampled Parameters: {'n_estimators': 112, 'learning_rate': 0.08154830386924548, 'max_depth': 9, 'max_features': None}\n",
      "Mean Score: 0.9100888656560376\n",
      "Iteration 3/200\n",
      "Sampled Parameters: {'n_estimators': 106, 'learning_rate': 0.01086711759230214, 'max_depth': 13, 'max_features': 'sqrt'}\n"
     ]
    }
   ],
   "source": [
    "# Perform multiple iterations of the hyperparameter tuning\n",
    "for i in range(n_iter):\n",
    "    print(f\"Iteration {i+1}/{n_iter}\")\n",
    "\n",
    "    # Sample hyperparameters from the ranges\n",
    "    params = {\n",
    "        'n_estimators': np.random.randint(param_ranges['n_estimators'][0], param_ranges['n_estimators'][1]),\n",
    "        'learning_rate': np.random.uniform(param_ranges['learning_rate'][0], param_ranges['learning_rate'][1]),\n",
    "        'max_depth': np.random.randint(param_ranges['max_depth'][0], param_ranges['max_depth'][1]),\n",
    "        'max_features': np.random.choice(param_ranges['max_features'])\n",
    "    }\n",
    "\n",
    "    print(f\"Sampled Parameters: {params}\")\n",
    "\n",
    "    # Create the GradientBoostingClassifier model\n",
    "    model = GradientBoostingClassifier(**params)\n",
    "\n",
    "    # # Perform cross-validation\n",
    "    # scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "\n",
    "    # # Calculate the mean score\n",
    "    # mean_score = np.mean(scores)\n",
    "\n",
    "    # print(f\"Mean Score: {mean_score}\")\n",
    "\n",
    "    # Fit the classifier on the training data\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get the feature importances\n",
    "    feature_importances = model.feature_importances_\n",
    "\n",
    "    # Sort the features by importance\n",
    "    sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "    # Select the top k features\n",
    "    k = 10\n",
    "    selected_features = X_train.columns[sorted_indices[:k]]\n",
    "\n",
    "    # Use only the selected features for training and testing\n",
    "    X_train_selected = X_train_scaled[:, sorted_indices[:k]]\n",
    "    X_test_selected = X_test_scaled[:, sorted_indices[:k]]\n",
    "\n",
    "    # Create a new model instance for final evaluation\n",
    "    final_model = GradientBoostingClassifier(**params)\n",
    "    final_model.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Evaluate the model on the testing data\n",
    "    mean_cv_score = final_model.score(X_test_selected, y_test)\n",
    "\n",
    "    print(f\"Mean Score: {mean_cv_score}\")\n",
    "\n",
    "    # Add the results to the results dataframe\n",
    "    # Create a new DataFrame with the results\n",
    "    new_row = pd.DataFrame([{\n",
    "        'n_estimators': params['n_estimators'],\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'max_depth': params['max_depth'],\n",
    "        'max_features': params['max_features'],\n",
    "        'mean_test_score': mean_cv_score\n",
    "    }])\n",
    "\n",
    "    # Concatenate the new row with the existing results_df\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(results_df['learning_rate'], results_history['n_estimators'], c=results_history['mean_test_score'], cmap='viridis', marker='x')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('N - Estimators')\n",
    "plt.title('Random Search Mean Cross-Validation Accuracy')\n",
    "plt.colorbar(label='Mean Test Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new model with the best hyperparameters\n",
    "gradient_boosting_regressor = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "#Train the model\n",
    "gradient_boosting_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Predict the target variable\n",
    "y_pred = gradient_boosting_regressor.predict(X_test_scaled)\n",
    "\n",
    "#Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R2 Score: {r2}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "#Viualize the predictions vs true values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Predictions vs True Values')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
